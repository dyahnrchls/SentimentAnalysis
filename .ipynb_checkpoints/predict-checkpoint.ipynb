{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8156e6e8-8d40-423a-8288-067bdfc616c6",
   "metadata": {},
   "source": [
    "# **Deep Convolutional Neural Network for Sentiment Analysis**\n",
    "\n",
    "Sebelummnya saya melakukan eksplorasi terhadap dua dataset yaitu **\"Spotify HUGE database - daily charts over 3 years\"** dan **\"World Vaccine Progress\"**. Lalu saya memutuskan untuk memilih *text mining*  karena *feasibility* untuk datasetnya mudah dicari dan *feature-feature* atau kolom yang ada pada data set tersebut biasanya bervariasi namun mudah dipahami.\n",
    "\n",
    "Dalam kegiatan ini saya akan meriset ulang(*rework*) metode yang dijelaskan di artikel kemudian merubah datasetnya ke dataset yang lain, dan memberikan sedikit tambahan visualisasi.  Artikel yang digunakan adalah [https://machinelearningmastery.com/develop-word-embedding-model-predicting-movie-review-sentiment/](https://machinelearningmastery.com/develop-word-embedding-model-predicting-movie-review-sentiment/)  dan data set yang akan digunakan adalah [https://www.kaggle.com/yash612/stockmarket-sentiment-dataset](https://www.kaggle.com/yash612/stockmarket-sentiment-dataset)**.\n",
    "\n",
    "Topik yang saya pilih adalah mengenai adalah *sentiment analysis* yaitu Natural Language Processing (NPL) memprediksi tulisan yang positif dan negatif. Artikel ini mengembangkan model *word embedding* untuk *neural networks* untuk mengklasifikasi ulasan pada film. \n",
    "\n",
    "Dataset yang saya gunakan saat ini mengenai berita saham beberapa tweets yang menangani berita ekonomi. Memiliki 2106 ulasan negatif dengan simbol (-1) dan 3685 ulasan positif dengan simbol (1)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7b4e902-82d3-4964-948b-7d6881383da9",
   "metadata": {},
   "source": [
    "1. *Cleaning dataset* (Membersihkan data set) yang ada pada file **cleaning_data.ipynb** \n",
    "Saya membersihkan dataset yang sebelumnya format .csv ke .txt, dengan 1 file .txt per ulasan serta memisahkan antara ulasan yang positif dan negatif.\n",
    "Saya juga membersihkan data dengan cara:\n",
    "\t-   Mengkonversi seluruh huruf ditulisan menjadi huruf kecil.\n",
    "\t-   Menambahkan *white space* disekitar titik, koma, dll.\n",
    "\t-   Memisahkan tulisan 1 kalimat 1 baris di .txt."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfc26289-1bce-418d-9e2b-6facabe43bd1",
   "metadata": {},
   "source": [
    "2. Melakukan eksplorasi\n",
    "\t- Melihat jumlah ulasan yang positif dan negatif.\n",
    "\t- Melihat jumlah kata yang ada.\n",
    "\t- Melihat rata-rata kata yang ada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c4ec235f-7e1d-4fdd-9c8b-d6e71401f7b3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>5791.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.272664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.962192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Sentiment\n",
       "count  5791.000000\n",
       "mean      0.272664\n",
       "std       0.962192\n",
       "min      -1.000000\n",
       "25%      -1.000000\n",
       "50%       1.000000\n",
       "75%       1.000000\n",
       "max       1.000000"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# EXPLORATION & READ \n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "\n",
    "data = pd.read_csv('stock_data.csv', encoding='utf-8')\n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ff89c11e-8be3-4ec6-8879-df03c1eeb1a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Text', 'Sentiment'], dtype='object')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bda7d183-d41b-401e-a9d2-c8b2c2e9e59c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Kickers on my watchlist XIDE TIT SOQ PNK CPW B...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>user: AAP MOVIE. 55% return for the FEA/GEED i...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>user I'd be afraid to short AMZN - they are lo...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MNTA Over 12.00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>OI  Over 21.37</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text  Sentiment\n",
       "0  Kickers on my watchlist XIDE TIT SOQ PNK CPW B...          1\n",
       "1  user: AAP MOVIE. 55% return for the FEA/GEED i...          1\n",
       "2  user I'd be afraid to short AMZN - they are lo...          1\n",
       "3                                  MNTA Over 12.00            1\n",
       "4                                   OI  Over 21.37            1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3490e605-f3ee-48cf-b30d-1e5ed38241c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5791"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['Text'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7e69b094-4dc7-4c56-b3d6-a872f00eab6f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3685"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['Text'][data['Sentiment'] == 1].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fa2c21d7-bb89-4bdb-9288-5857528a6fb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2106"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data.groupby(['Sentiment']).groups[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "54879d4b-640b-4439-a832-2597d1b74204",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1</td>\n",
       "      <td>2106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>3685</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Sentiment  Text\n",
       "0         -1  2106\n",
       "1          1  3685"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.groupby('Sentiment', as_index=False).agg({\"Text\": \"count\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e70cadab-fc37-4fb6-ac9b-f6224d381c28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           Text\n",
      "Sentiment      \n",
      "-1         2106\n",
      " 1         3685\n"
     ]
    }
   ],
   "source": [
    "counts = data.groupby(['Sentiment']).count()\n",
    "print(counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8f9c0bc3-ebf9-46d0-b506-38a9e4200d94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text    5791\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(counts.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7948a358-f50f-4e9c-bba1-aa072fd4ad55",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc[data['Sentiment'] == 1, ['Sentiment']] = 'Positive'\n",
    "data.loc[data['Sentiment'] == -1, ['Sentiment']] = 'Negative'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "556bca25-df41-4dbe-961b-8300dd85845b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['Negative', 'Positive'])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.groupby(['Sentiment']).groups.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b6b875fd-4ef0-44d6-bd4f-e881797fbadd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXPLORATION\n",
    "\n",
    "from string import punctuation\n",
    "from os import listdir\n",
    "from numpy import array\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Embedding\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "import numpy\n",
    "\n",
    "# load doc into memory\n",
    "def load_doc(filename):\n",
    "    # open the file as read only\n",
    "    file = open(filename, 'r', errors=\"ignore\")\n",
    "    # read all text\n",
    "    text = file.read()\n",
    "    # close the file\n",
    "    file.close()\n",
    "    return text\n",
    " \n",
    "# turn a doc into clean tokens\n",
    "def clean_doc(doc, vocab):\n",
    "    # split into tokens by white space\n",
    "    tokens = doc.split()\n",
    "    # remove punctuation from each token\n",
    "    table = str.maketrans('', '', punctuation)\n",
    "    tokens = [w.translate(table) for w in tokens]\n",
    "    # filter out tokens not in vocab\n",
    "    tokens = [w for w in tokens if w in vocab]\n",
    "    tokens = ' '.join(tokens)\n",
    "    return tokens\n",
    " \n",
    "# load all docs in a directory\n",
    "def process_docs(directory, vocab):\n",
    "    documents = list()\n",
    "    # walk through all files in the folder\n",
    "    for filename in listdir(directory):\n",
    "        # create the full path of the file to open\n",
    "        path = directory + '/' + filename\n",
    "        # load the doc\n",
    "        doc = load_doc(path)\n",
    "        # clean doc\n",
    "        tokens = clean_doc(doc, vocab)\n",
    "        # add to list\n",
    "        documents.append(tokens)\n",
    "    return documents\n",
    " \n",
    "# load the vocabulary\n",
    "vocab_filename = 'data/all/vocab.txt'\n",
    "vocab = load_doc(vocab_filename)\n",
    "vocab = vocab.split()\n",
    "vocab = set(vocab)\n",
    " \n",
    "# load all training reviews\n",
    "positive_docs = process_docs('data/train/pos', vocab)\n",
    "negative_docs = process_docs('data/train/neg', vocab)\n",
    "train_docs = negative_docs + positive_docs\n",
    " \n",
    "# create the tokenizer\n",
    "tokenizer = Tokenizer()\n",
    "# fit the tokenizer on the documents\n",
    "tokenizer.fit_on_texts(train_docs)\n",
    " \n",
    "# sequence encode\n",
    "encoded_docs = tokenizer.texts_to_sequences(train_docs)\n",
    "# pad sequences\n",
    "max_length = max([len(s.split()) for s in train_docs])\n",
    "Xtrain = pad_sequences(encoded_docs, maxlen=max_length, padding='post')\n",
    "# define training labels\n",
    "ytrain = array([0 for _ in range(2316)] + [1 for _ in range(2317)])\n",
    " \n",
    "# # load all test reviews\n",
    "positive_docs = process_docs('data/test/pos', vocab)\n",
    "negative_docs = process_docs('data/test/neg', vocab)\n",
    "test_docs = negative_docs + positive_docs\n",
    "# sequence encode\n",
    "encoded_docs = tokenizer.texts_to_sequences(test_docs)\n",
    "# pad sequences\n",
    "Xtest = pad_sequences(encoded_docs, maxlen=max_length, padding='post')\n",
    "# define test labels\n",
    "ytest = array([0 for _ in range(579)] + [1 for _ in range(579)])\n",
    " \n",
    "# define vocabulary size (largest integer value)\n",
    "vocab_size = len(tokenizer.word_index) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "68637cfa-a59b-4cf9-a365-76bb9dc0dc7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data\n",
    "X = numpy.concatenate((Xtrain, Xtest), axis=0)\n",
    "y = numpy.concatenate((ytrain, ytest), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0fb58cae-2bab-4411-b974-c97589b593f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data: \n",
      "(5791, 21)\n",
      "(5791,)\n"
     ]
    }
   ],
   "source": [
    "# summarize size\n",
    "print(\"Training data: \")\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b9442309-080e-454c-a134-33790eb5f201",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes: \n",
      "[0 1]\n"
     ]
    }
   ],
   "source": [
    "# Summarize number of classes\n",
    "print(\"Classes: \")\n",
    "print(numpy.unique(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2e911ac7-2462-48fd-9d00-60a4431a1024",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words: \n",
      "3671\n"
     ]
    }
   ],
   "source": [
    "# Summarize number of words\n",
    "print(\"Number of words: \")\n",
    "print(len(numpy.unique(numpy.hstack(X))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "315cfd8b-936b-43df-8734-2ebc4ddcd0b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review length: \n",
      "Mean 21.00 words (0.000000)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as pyplot\n",
    "# Summarize review length\n",
    "print(\"Review length: \")\n",
    "result = list(map(len, X))\n",
    "print(\"Mean %.2f words (%f)\" % (numpy.mean(result), numpy.std(result)))\n",
    "# plot review length as a boxplot and histogram\n",
    "pyplot.subplot(121)\n",
    "pyplot.boxplot(result)\n",
    "pyplot.subplot(122)\n",
    "pyplot.hist(result)\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fe7762e-4870-46fd-ae23-0ac872f433df",
   "metadata": {},
   "source": [
    "\t- Melihat jumlah kata-kata yang ada pada data\n",
    "\t- Melihat 50 kata yang paling banyak digunakan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c985b852-8a32-49e3-9db8-8d9105010198",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8341\n",
      "[('aap', 922), ('httpst', 690), ('user', 641), ('short', 436), ('today', 323), ('volume', 303), ('like', 278), ('day', 269), ('long', 257), ('good', 229), ('stock', 224), ('goog', 212), ('new', 208), ('watch', 205), ('bac', 204), ('stop', 201), ('still', 199), ('nice', 196), ('back', 187), ('buy', 182), ('next', 181), ('move', 180), ('higher', 176), ('coronavirus', 164), ('see', 162), ('market', 159), ('ong', 156), ('one', 153), ('sensex', 150), ('triangle', 147), ('time', 147), ('trade', 143), ('close', 143), ('week', 142), ('stocks', 142), ('nifty', 140), ('weekly', 136), ('could', 134), ('break', 131), ('looking', 131), ('breakout', 127), ('big', 127), ('support', 124), ('go', 122), ('going', 120), ('bullish', 119), ('nfx', 117), ('last', 116), ('looks', 115), ('green', 113)]\n"
     ]
    }
   ],
   "source": [
    "# PREPARATION\n",
    "\n",
    "from string import punctuation\n",
    "from os import listdir\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    " \n",
    "# load doc into memory\n",
    "def load_doc(filename):\n",
    "    # open the file as read only\n",
    "    file = open(filename, 'r', encoding=\"utf8\")\n",
    "    # read all text\n",
    "    text = file.read()\n",
    "    # close the file\n",
    "    file.close()\n",
    "    return text\n",
    " \n",
    "# turn a doc into clean tokens\n",
    "def clean_doc(doc):\n",
    "    # split into tokens by white space\n",
    "    tokens = doc.split()\n",
    "    # remove punctuation from each token\n",
    "    table = str.maketrans('', '', punctuation)\n",
    "    tokens = [w.translate(table) for w in tokens]\n",
    "    # remove remaining tokens that are not alphabetic\n",
    "    tokens = [word for word in tokens if word.isalpha()]\n",
    "    # filter out stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [w for w in tokens if not w in stop_words]\n",
    "    # filter out short tokens\n",
    "    tokens = [word for word in tokens if len(word) > 1]\n",
    "    return tokens\n",
    " \n",
    "# load doc and add to vocab\n",
    "def add_doc_to_vocab(filename, vocab):\n",
    "    # load doc\n",
    "    doc = load_doc(filename)\n",
    "    # clean doc\n",
    "    tokens = clean_doc(doc)\n",
    "    # update counts\n",
    "    vocab.update(tokens)\n",
    " \n",
    "# load all docs in a directory\n",
    "def process_docs(directory, vocab, is_trian):\n",
    "    # walk through all files in the folder\n",
    "    for filename in listdir(directory):\n",
    "        # skip any reviews in the test set\n",
    "        if is_trian and filename.startswith('cv9'):\n",
    "            continue\n",
    "        if not is_trian and not filename.startswith('cv9'):\n",
    "            continue\n",
    "        # create the full path of the file to open\n",
    "        path = directory + '/' + filename\n",
    "        # add doc to vocab\n",
    "        add_doc_to_vocab(path, vocab)\n",
    " \n",
    "# define vocab\n",
    "vocab = Counter()\n",
    "# add all docs to vocab\n",
    "process_docs('data/all/neg', vocab, True)\n",
    "process_docs('data/all/pos', vocab, True)\n",
    "# print the size of the vocab\n",
    "print(len(vocab))\n",
    "# print the top words in the vocab\n",
    "print(vocab.most_common(50))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f586125b-1486-456e-ba2d-a63fd3c13ee3",
   "metadata": {},
   "source": [
    "\t- Menghapus kata-kata yang hanya dipakai 1-2 kali dalam ulasan.\n",
    "\t- Melihat kata-kata yang ada 2 atau lebih dalam ulasan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "eb031fd6-e531-4c80-91d0-9c9a767c1114",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3995\n"
     ]
    }
   ],
   "source": [
    "# keep tokens with a min occurrence\n",
    "min_occurane = 2\n",
    "tokens = [k for k,c in vocab.items() if c >= min_occurane]\n",
    "print(len(tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "370a1025-3bba-4f8b-9c45-81f59641fe6d",
   "metadata": {},
   "source": [
    "\t- Menyimpan kata-kata tersebut dalam vocab.txt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "db41ccf1-2e32-4f4a-8609-861172a8f35a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save list to file\n",
    "def save_list(lines, filename):\n",
    "    # convert lines to a single blob of text\n",
    "    data = '\\n'.join(lines)\n",
    "    # open file\n",
    "    file = open(filename, 'w')\n",
    "    # write text\n",
    "    file.write(data)\n",
    "    # close file\n",
    "    file.close()\n",
    " \n",
    "# save tokens to a vocabulary file\n",
    "save_list(tokens, 'data/all/vocab.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7fe9430-6736-4d41-9a31-f30c996ae391",
   "metadata": {},
   "source": [
    "3. Melakukan prediksi model menggunakan embedding layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d49fe5a2-9f01-4c1f-84d0-b6d826e81298",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_12\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_12 (Embedding)     (None, 21, 100)           367100    \n",
      "_________________________________________________________________\n",
      "conv1d_12 (Conv1D)           (None, 14, 32)            25632     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_12 (MaxPooling (None, 7, 32)             0         \n",
      "_________________________________________________________________\n",
      "flatten_12 (Flatten)         (None, 224)               0         \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             (None, 10)                2250      \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 394,993\n",
      "Trainable params: 394,993\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/10\n",
      "145/145 - 4s - loss: 0.6923 - accuracy: 0.5139\n",
      "Epoch 2/10\n",
      "145/145 - 1s - loss: 0.5563 - accuracy: 0.7408\n",
      "Epoch 3/10\n",
      "145/145 - 1s - loss: 0.2744 - accuracy: 0.8962\n",
      "Epoch 4/10\n",
      "145/145 - 2s - loss: 0.1327 - accuracy: 0.9566\n",
      "Epoch 5/10\n",
      "145/145 - 1s - loss: 0.0782 - accuracy: 0.9735\n",
      "Epoch 6/10\n",
      "145/145 - 1s - loss: 0.0506 - accuracy: 0.9834\n",
      "Epoch 7/10\n",
      "145/145 - 1s - loss: 0.0354 - accuracy: 0.9875\n",
      "Epoch 8/10\n",
      "145/145 - 2s - loss: 0.0307 - accuracy: 0.9907\n",
      "Epoch 9/10\n",
      "145/145 - 2s - loss: 0.0269 - accuracy: 0.9918\n",
      "Epoch 10/10\n",
      "145/145 - 2s - loss: 0.0189 - accuracy: 0.9937\n",
      "Test Accuracy: 56.994820\n"
     ]
    }
   ],
   "source": [
    "# EMBEDDING LAYER\n",
    "\n",
    "from string import punctuation\n",
    "from os import listdir\n",
    "from numpy import array\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Embedding\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    " \n",
    "# load doc into memory\n",
    "def load_doc(filename):\n",
    "    # open the file as read only\n",
    "    file = open(filename, 'r', errors=\"ignore\")\n",
    "    # read all text\n",
    "    text = file.read()\n",
    "    # close the file\n",
    "    file.close()\n",
    "    return text\n",
    " \n",
    "# turn a doc into clean tokens\n",
    "def clean_doc(doc, vocab):\n",
    "    # split into tokens by white space\n",
    "    tokens = doc.split()\n",
    "    # remove punctuation from each token\n",
    "    table = str.maketrans('', '', punctuation)\n",
    "    tokens = [w.translate(table) for w in tokens]\n",
    "    # filter out tokens not in vocab\n",
    "    tokens = [w for w in tokens if w in vocab]\n",
    "    tokens = ' '.join(tokens)\n",
    "    return tokens\n",
    " \n",
    "# load all docs in a directory\n",
    "def process_docs(directory, vocab):\n",
    "    documents = list()\n",
    "    # walk through all files in the folder\n",
    "    for filename in listdir(directory):\n",
    "        # create the full path of the file to open\n",
    "        path = directory + '/' + filename\n",
    "        # load the doc\n",
    "        doc = load_doc(path)\n",
    "        # clean doc\n",
    "        tokens = clean_doc(doc, vocab)\n",
    "        # add to list\n",
    "        documents.append(tokens)\n",
    "    return documents\n",
    " \n",
    "# load the vocabulary\n",
    "vocab_filename = 'data/all/vocab.txt'\n",
    "vocab = load_doc(vocab_filename)\n",
    "vocab = vocab.split()\n",
    "vocab = set(vocab)\n",
    " \n",
    "# load all training reviews\n",
    "positive_docs = process_docs('data/train/pos', vocab)\n",
    "negative_docs = process_docs('data/train/neg', vocab)\n",
    "train_docs = negative_docs + positive_docs\n",
    " \n",
    "# create the tokenizer\n",
    "tokenizer = Tokenizer()\n",
    "# fit the tokenizer on the documents\n",
    "tokenizer.fit_on_texts(train_docs)\n",
    " \n",
    "# sequence encode\n",
    "encoded_docs = tokenizer.texts_to_sequences(train_docs)\n",
    "# pad sequences\n",
    "max_length = max([len(s.split()) for s in train_docs])\n",
    "Xtrain = pad_sequences(encoded_docs, maxlen=max_length, padding='post')\n",
    "# define training labels\n",
    "ytrain = array([0 for _ in range(2316)] + [1 for _ in range(2317)])\n",
    " \n",
    "# # load all test reviews\n",
    "positive_docs = process_docs('data/test/pos', vocab)\n",
    "negative_docs = process_docs('data/test/neg', vocab)\n",
    "test_docs = negative_docs + positive_docs\n",
    "# sequence encode\n",
    "encoded_docs = tokenizer.texts_to_sequences(test_docs)\n",
    "# pad sequences\n",
    "Xtest = pad_sequences(encoded_docs, maxlen=max_length, padding='post')\n",
    "# define test labels\n",
    "ytest = array([0 for _ in range(579)] + [1 for _ in range(579)])\n",
    " \n",
    "# define vocabulary size (largest integer value)\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    " \n",
    "# define model\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 100, input_length=max_length))\n",
    "model.add(Conv1D(filters=32, kernel_size=8, activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(10, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "print(model.summary())\n",
    "# compile network\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# fit network\n",
    "model.fit(Xtrain, ytrain, epochs=10, verbose=2)\n",
    "# evaluate\n",
    "loss, acc = model.evaluate(Xtest, ytest, verbose=0)\n",
    "print('Test Accuracy: %f' % (acc*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "071c5d59-a954-41df-9028-ff825ccc2969",
   "metadata": {},
   "source": [
    "4. Membuat word2vec, yaitu mengkonversikan tulisan ke dalam *vector representative*, konversi ini dibuat perkata untuk melihat relasi, kesamaan dari kata-kata yang didapat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "69b317ce-e235-4f47-9e62-fdd08d6fb7c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training sentences: 6799\n",
      "Vocabulary size: 3670\n"
     ]
    }
   ],
   "source": [
    " # WORD2VEC MODEL\n",
    "\n",
    "from string import punctuation\n",
    "from os import listdir\n",
    "from gensim.models import Word2Vec\n",
    " \n",
    "# load doc into memory\n",
    "def load_doc(filename):\n",
    "    # open the file as read only\n",
    "    file = open(filename, 'r', errors=\"ignore\")\n",
    "    # read all text\n",
    "    text = file.read()\n",
    "    # close the file\n",
    "    file.close()\n",
    "    return text\n",
    " \n",
    "# turn a doc into clean tokens\n",
    "def doc_to_clean_lines(doc, vocab):\n",
    "    clean_lines = list()\n",
    "    lines = doc.splitlines()\n",
    "    for line in lines:\n",
    "        # split into tokens by white space\n",
    "        tokens = line.split()\n",
    "        # remove punctuation from each token\n",
    "        table = str.maketrans('', '', punctuation)\n",
    "        tokens = [w.translate(table) for w in tokens]\n",
    "        # filter out tokens not in vocab\n",
    "        tokens = [w for w in tokens if w in vocab]\n",
    "        clean_lines.append(tokens)\n",
    "    return clean_lines\n",
    " \n",
    "# load all docs in a directory\n",
    "def process_docs(directory, vocab):\n",
    "    lines = list()\n",
    "    # walk through all files in the folder\n",
    "    for filename in listdir(directory):\n",
    "        # create the full path of the file to open\n",
    "        path = directory + '/' + filename\n",
    "        # load and clean the doc\n",
    "        doc = load_doc(path)\n",
    "        doc_lines = doc_to_clean_lines(doc, vocab)\n",
    "        # add lines to list\n",
    "        lines += doc_lines\n",
    "    return lines\n",
    " \n",
    "# load the vocabulary\n",
    "vocab_filename = 'data/all/vocab.txt'\n",
    "vocab = load_doc(vocab_filename)\n",
    "vocab = vocab.split()\n",
    "vocab = set(vocab)\n",
    " \n",
    "# load training data\n",
    "positive_docs = process_docs('data/train/pos', vocab)\n",
    "negative_docs = process_docs('data/train/neg', vocab)\n",
    "sentences = negative_docs + positive_docs\n",
    "print('Total training sentences: %d' % len(sentences))\n",
    " \n",
    "# train word2vec model\n",
    "model = Word2Vec(sentences, vector_size=100, window=5, workers=8, min_count=1)\n",
    "# summarize vocabulary size in model\n",
    "words = list(model.wv.index_to_key)\n",
    "print('Vocabulary size: %d' % len(words))\n",
    " \n",
    "# save model in ASCII (word2vec) format\n",
    "filename = 'embedding_word2vec.txt'\n",
    "model.wv.save_word2vec_format(filename, binary=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7533c864-3070-4e6b-9372-c41ffc48e3e7",
   "metadata": {},
   "source": [
    "5. Melakukan prediksi model dengan pre-trained embedding menggunakan word2vec yang telah dibuat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c6c8f70e-4d3d-46e6-a6f8-b3f436b4d674",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 21, 100)           367100    \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 17, 128)           64128     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 8, 128)            0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 1025      \n",
      "=================================================================\n",
      "Total params: 432,253\n",
      "Trainable params: 65,153\n",
      "Non-trainable params: 367,100\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/10\n",
      "145/145 - 2s - loss: 0.6959 - accuracy: 0.5049\n",
      "Epoch 2/10\n",
      "145/145 - 1s - loss: 0.6912 - accuracy: 0.5219\n",
      "Epoch 3/10\n",
      "145/145 - 1s - loss: 0.6900 - accuracy: 0.5256\n",
      "Epoch 4/10\n",
      "145/145 - 1s - loss: 0.6885 - accuracy: 0.5327\n",
      "Epoch 5/10\n",
      "145/145 - 1s - loss: 0.6888 - accuracy: 0.5338\n",
      "Epoch 6/10\n",
      "145/145 - 1s - loss: 0.6867 - accuracy: 0.5398\n",
      "Epoch 7/10\n",
      "145/145 - 1s - loss: 0.6851 - accuracy: 0.5459\n",
      "Epoch 8/10\n",
      "145/145 - 1s - loss: 0.6855 - accuracy: 0.5459\n",
      "Epoch 9/10\n",
      "145/145 - 1s - loss: 0.6850 - accuracy: 0.5519\n",
      "Epoch 10/10\n",
      "145/145 - 1s - loss: 0.6828 - accuracy: 0.5558\n",
      "Test Accuracy: 52.245253\n"
     ]
    }
   ],
   "source": [
    "# PRE-TRAINED EMBEDDING WITH WORD2VEC\n",
    "\n",
    "from string import punctuation\n",
    "from os import listdir\n",
    "from numpy import array\n",
    "from numpy import asarray\n",
    "from numpy import zeros\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Embedding\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    " \n",
    "# load doc into memory\n",
    "def load_doc(filename):\n",
    "    # open the file as read only\n",
    "    file = open(filename, 'r', errors=\"ignore\")\n",
    "    # read all text\n",
    "    text = file.read()\n",
    "    # close the file\n",
    "    file.close()\n",
    "    return text\n",
    " \n",
    "# turn a doc into clean tokens\n",
    "def clean_doc(doc, vocab):\n",
    "    # split into tokens by white space\n",
    "    tokens = doc.split()\n",
    "    # remove punctuation from each token\n",
    "    table = str.maketrans('', '', punctuation)\n",
    "    tokens = [w.translate(table) for w in tokens]\n",
    "    # filter out tokens not in vocab\n",
    "    tokens = [w for w in tokens if w in vocab]\n",
    "    tokens = ' '.join(tokens)\n",
    "    return tokens\n",
    " \n",
    "# load all docs in a directory\n",
    "def process_docs(directory, vocab):\n",
    "    documents = list()\n",
    "    # walk through all files in the folder\n",
    "    for filename in listdir(directory):\n",
    "        # create the full path of the file to open\n",
    "        path = directory + '/' + filename\n",
    "        # load the doc\n",
    "        doc = load_doc(path)\n",
    "        # clean doc\n",
    "        tokens = clean_doc(doc, vocab)\n",
    "        # add to list\n",
    "        documents.append(tokens)\n",
    "    return documents\n",
    " \n",
    "# load embedding as a dict\n",
    "def load_embedding(filename):\n",
    "    # load embedding into memory, skip first line\n",
    "    file = open(filename,'r')\n",
    "    lines = file.readlines()[1:]\n",
    "    file.close()\n",
    "    # create a map of words to vectors\n",
    "    embedding = dict()\n",
    "    for line in lines:\n",
    "        parts = line.split()\n",
    "        # key is string word, value is numpy array for vector\n",
    "        embedding[parts[0]] = asarray(parts[1:], dtype='float32')\n",
    "    return embedding\n",
    " \n",
    "# create a weight matrix for the Embedding layer from a loaded embedding\n",
    "def get_weight_matrix(embedding, vocab):\n",
    "    # total vocabulary size plus 0 for unknown words\n",
    "    vocab_size = len(vocab) + 1\n",
    "    # define weight matrix dimensions with all 0\n",
    "    weight_matrix = zeros((vocab_size, 100))\n",
    "    # step vocab, store vectors using the Tokenizer's integer mapping\n",
    "    for word, i in vocab.items():\n",
    "        weight_matrix[i] = embedding.get(word)\n",
    "    return weight_matrix\n",
    " \n",
    "# load the vocabulary\n",
    "vocab_filename = 'data/all/vocab.txt'\n",
    "vocab = load_doc(vocab_filename)\n",
    "vocab = vocab.split()\n",
    "vocab = set(vocab)\n",
    " \n",
    "# load all training reviews\n",
    "positive_docs = process_docs('data/train/pos', vocab)\n",
    "negative_docs = process_docs('data/train/neg', vocab)\n",
    "train_docs = negative_docs + positive_docs\n",
    " \n",
    "# create the tokenizer\n",
    "tokenizer = Tokenizer()\n",
    "# fit the tokenizer on the documents\n",
    "tokenizer.fit_on_texts(train_docs)\n",
    " \n",
    "# sequence encode\n",
    "encoded_docs = tokenizer.texts_to_sequences(train_docs)\n",
    "# pad sequences\n",
    "max_length = max([len(s.split()) for s in train_docs])\n",
    "Xtrain = pad_sequences(encoded_docs, maxlen=max_length, padding='post')\n",
    "# define training labels\n",
    "ytrain = array([0 for _ in range(2316)] + [1 for _ in range(2317)])\n",
    " \n",
    "# load all test reviews\n",
    "positive_docs = process_docs('data/test/pos', vocab)\n",
    "negative_docs = process_docs('data/test/neg', vocab)\n",
    "test_docs = negative_docs + positive_docs\n",
    "# sequence encode\n",
    "encoded_docs = tokenizer.texts_to_sequences(test_docs)\n",
    "# pad sequences\n",
    "Xtest = pad_sequences(encoded_docs, maxlen=max_length, padding='post')\n",
    "# define test labels\n",
    "ytest = array([0 for _ in range(579)] + [1 for _ in range(579)])\n",
    " \n",
    "# define vocabulary size (largest integer value)\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    " \n",
    "# load embedding from file\n",
    "raw_embedding = load_embedding('embedding_word2vec.txt')\n",
    "# get vectors in the right order\n",
    "embedding_vectors = get_weight_matrix(raw_embedding, tokenizer.word_index)\n",
    "# create the embedding layer\n",
    "embedding_layer = Embedding(vocab_size, 100, weights=[embedding_vectors], input_length=max_length, trainable=False)\n",
    " \n",
    "# define model\n",
    "model = Sequential()\n",
    "model.add(embedding_layer)\n",
    "model.add(Conv1D(filters=128, kernel_size=5, activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "print(model.summary())\n",
    "# compile network\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# fit network\n",
    "model.fit(Xtrain, ytrain, epochs=10, verbose=2)\n",
    "# evaluate\n",
    "loss, acc = model.evaluate(Xtest, ytest, verbose=0)\n",
    "print('Test Accuracy: %f' % (acc*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4b7be9a-842b-476a-b89c-038a9118488d",
   "metadata": {},
   "source": [
    "6. Melakukan prediksi model dengan pre-trained embedding menggunakan glove dari google yang memiliki variasi kata lebih banyak."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a4aa69d9-49eb-4e5a-a56b-7c616fb7adf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 21, 100)           367100    \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 17, 128)           64128     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 8, 128)            0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 1025      \n",
      "=================================================================\n",
      "Total params: 432,253\n",
      "Trainable params: 65,153\n",
      "Non-trainable params: 367,100\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/10\n",
      "145/145 - 2s - loss: 0.6864 - accuracy: 0.5649\n",
      "Epoch 2/10\n",
      "145/145 - 1s - loss: 0.5839 - accuracy: 0.7011\n",
      "Epoch 3/10\n",
      "145/145 - 1s - loss: 0.4965 - accuracy: 0.7719\n",
      "Epoch 4/10\n",
      "145/145 - 1s - loss: 0.4089 - accuracy: 0.8353\n",
      "Epoch 5/10\n",
      "145/145 - 1s - loss: 0.3286 - accuracy: 0.8873\n",
      "Epoch 6/10\n",
      "145/145 - 1s - loss: 0.2540 - accuracy: 0.9266\n",
      "Epoch 7/10\n",
      "145/145 - 1s - loss: 0.2046 - accuracy: 0.9478\n",
      "Epoch 8/10\n",
      "145/145 - 1s - loss: 0.1602 - accuracy: 0.9631\n",
      "Epoch 9/10\n",
      "145/145 - 1s - loss: 0.1318 - accuracy: 0.9685\n",
      "Epoch 10/10\n",
      "145/145 - 1s - loss: 0.1121 - accuracy: 0.9754\n",
      "Test Accuracy: 53.886008\n"
     ]
    }
   ],
   "source": [
    " # PRE-TRAINED EMBEDDING WITH GLOVE\n",
    "\n",
    "from string import punctuation\n",
    "from os import listdir\n",
    "from numpy import array\n",
    "from numpy import asarray\n",
    "from numpy import zeros\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Embedding\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    " \n",
    "# load doc into memory\n",
    "def load_doc(filename):\n",
    "    # open the file as read only\n",
    "    file = open(filename, 'r', errors=\"ignore\")\n",
    "    # read all text\n",
    "    text = file.read()\n",
    "    # close the file\n",
    "    file.close()\n",
    "    return text\n",
    " \n",
    "# turn a doc into clean tokens\n",
    "def clean_doc(doc, vocab):\n",
    "    # split into tokens by white space\n",
    "    tokens = doc.split()\n",
    "    # remove punctuation from each token\n",
    "    table = str.maketrans('', '', punctuation)\n",
    "    tokens = [w.translate(table) for w in tokens]\n",
    "    # filter out tokens not in vocab\n",
    "    tokens = [w for w in tokens if w in vocab]\n",
    "    tokens = ' '.join(tokens)\n",
    "    return tokens\n",
    " \n",
    "# load all docs in a directory\n",
    "def process_docs(directory, vocab):\n",
    "    documents = list()\n",
    "    # walk through all files in the folder\n",
    "    for filename in listdir(directory):\n",
    "        # skip any reviews in the test set\n",
    "        path = directory + '/' + filename\n",
    "        # load the doc\n",
    "        doc = load_doc(path)\n",
    "        # clean doc\n",
    "        tokens = clean_doc(doc, vocab)\n",
    "        # add to list\n",
    "        documents.append(tokens)\n",
    "    return documents\n",
    " \n",
    "# load embedding as a dict\n",
    "def load_embedding(filename):\n",
    "    # load embedding into memory, skip first line\n",
    "    file = open(filename,'r', encoding=\"utf8\")\n",
    "    lines = file.readlines()\n",
    "    file.close()\n",
    "    # create a map of words to vectors\n",
    "    embedding = dict()\n",
    "    for line in lines:\n",
    "        parts = line.split()\n",
    "        # key is string word, value is numpy array for vector\n",
    "        embedding[parts[0]] = asarray(parts[1:], dtype='float32')\n",
    "    return embedding\n",
    " \n",
    "# create a weight matrix for the Embedding layer from a loaded embedding\n",
    "def get_weight_matrix(embedding, vocab):\n",
    "    # total vocabulary size plus 0 for unknown words\n",
    "    vocab_size = len(vocab) + 1\n",
    "    # define weight matrix dimensions with all 0\n",
    "    weight_matrix = zeros((vocab_size, 100))\n",
    "    # step vocab, store vectors using the Tokenizer's integer mapping\n",
    "    for word, i in vocab.items():\n",
    "        vector = embedding.get(word)\n",
    "        if vector is not None:\n",
    "            weight_matrix[i] = vector\n",
    "    return weight_matrix\n",
    " \n",
    "# load the vocabulary\n",
    "vocab_filename = 'data/all/vocab.txt'\n",
    "vocab = load_doc(vocab_filename)\n",
    "vocab = vocab.split()\n",
    "vocab = set(vocab)\n",
    " \n",
    "# load all training reviews\n",
    "positive_docs = process_docs('data/train/pos', vocab)\n",
    "negative_docs = process_docs('data/train/neg', vocab)\n",
    "train_docs = negative_docs + positive_docs\n",
    " \n",
    "# create the tokenizer\n",
    "tokenizer = Tokenizer()\n",
    "# fit the tokenizer on the documents\n",
    "tokenizer.fit_on_texts(train_docs)\n",
    " \n",
    "# sequence encode\n",
    "encoded_docs = tokenizer.texts_to_sequences(train_docs)\n",
    "# pad sequences\n",
    "max_length = max([len(s.split()) for s in train_docs])\n",
    "Xtrain = pad_sequences(encoded_docs, maxlen=max_length, padding='post')\n",
    "# define training labels\n",
    "ytrain = array([0 for _ in range(2316)] + [1 for _ in range(2317)])\n",
    " \n",
    "# load all test reviews\n",
    "positive_docs = process_docs('data/test/pos', vocab)\n",
    "negative_docs = process_docs('data/test/neg', vocab)\n",
    "test_docs = negative_docs + positive_docs\n",
    "# sequence encode\n",
    "encoded_docs = tokenizer.texts_to_sequences(test_docs)\n",
    "# pad sequences\n",
    "Xtest = pad_sequences(encoded_docs, maxlen=max_length, padding='post')\n",
    "# define test labels\n",
    "ytest = array([0 for _ in range(579)] + [1 for _ in range(579)])\n",
    " \n",
    "# define vocabulary size (largest integer value)\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    " \n",
    "# load embedding from file\n",
    "raw_embedding = load_embedding('glove.6B.100d.txt')\n",
    "# get vectors in the right order\n",
    "embedding_vectors = get_weight_matrix(raw_embedding, tokenizer.word_index)\n",
    "# create the embedding layer\n",
    "embedding_layer = Embedding(vocab_size, 100, weights=[embedding_vectors], input_length=max_length, trainable=False)\n",
    " \n",
    "# define model\n",
    "model = Sequential()\n",
    "model.add(embedding_layer)\n",
    "model.add(Conv1D(filters=128, kernel_size=5, activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "print(model.summary())\n",
    "# compile network\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# fit network\n",
    "model.fit(Xtrain, ytrain, epochs=10, verbose=2)\n",
    "# evaluate\n",
    "loss, acc = model.evaluate(Xtest, ytest, verbose=0)\n",
    "print('Test Accuracy: %f' % (acc*100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
